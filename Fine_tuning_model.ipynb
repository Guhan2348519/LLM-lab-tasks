{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyq6hMYl6hcB/oFsPDLo6A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guhan2348519/LLM-lab-tasks/blob/main/Fine_tuning_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_vrpgnY1DCC"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model_name: ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
        "model_name = \"gpt2\"\n",
        "model_save_path = './model'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rByXvC-1mhL",
        "outputId": "e94a5aa1-20eb-491d-9ef2-de67a1625624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "configuration = GPT2Config.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "input_ids = tokenizer.encode(input_sequence, return_tensors='pt')\n",
        "attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "\n",
        "model = model.to(device)\n",
        "sample_outputs = model.generate(\n",
        "    input_ids.to(device),\n",
        "    attention_mask=attention_mask,\n",
        "    do_sample=True, max_length=120,\n",
        "    top_k=50, top_p=0.85,\n",
        "    num_return_sequences=3,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "nThWDq5L2Ioe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_recipes = pd.read_csv('/content/recipes_1000.csv')\n",
        "df_recipes.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def form_string(ingredient,instruction):\n",
        "    s = f\"<|startoftext|>Ingredients: {ingredient.strip()}. \" \\\n",
        "        f\"Instructions: {instruction.strip()}<|endoftext|>\"\n",
        "    return s\n",
        "\n",
        "data = df_recipes.apply(lambda x:form_string(\n",
        "    x['ingredients'], x['instructions']), axis=1).to_list()\n",
        "data[0]"
      ],
      "metadata": {
        "id": "gVrHsEJd20mt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c301ac93-a450-4b1c-8177-29f0d9cf3854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|startoftext|>Ingredients: blueberries, granulated sugar, vanilla yogurt, lemon juice. Instructions: Toss 2 cups berries with sugar. Let stand for 45 minutes, stirring occasionally. Transfer berry-sugar mixture to food processor. Add yogurt and process until smooth. Strain through fine sieve. Pour into baking pan (or transfer to ice cream maker and process according to manufacturers' directions). Freeze uncovered until edges are solid but centre is soft.  Transfer to processor and blend until smooth again. Return to pan and freeze until edges are solid. Transfer to processor and blend until smooth again. Fold in remaining 2 cups of blueberries. Pour into plastic mold and freeze overnight. Let soften slightly to serve.<|endoftext|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name,\n",
        "                                              bos_token='<|startoftext|>',\n",
        "                                              eos_token='<|endoftext|>',\n",
        "                                              unk_token='<|unknown|>',\n",
        ")"
      ],
      "metadata": {
        "id": "FGLQq-8228S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "batch_size = 2\n",
        "max_length = 180\n",
        "\n",
        "# Set the padding token to the EOS token\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Sample data\n",
        "data = [\"beef, salt, pepper\", \"chicken, garlic, herbs\", \"pasta, tomato, basil\", \"rice, beans, avocado\"]\n",
        "\n",
        "# Standard PyTorch approach of loading data using a Dataset class\n",
        "class RecipeDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for recipe in data:\n",
        "            encodings = tokenizer.encode_plus(recipe,\n",
        "                                              truncation=True,\n",
        "                                              padding='max_length',\n",
        "                                              max_length=max_length,\n",
        "                                              # return a PyTorch tensor\n",
        "                                              return_tensors='pt'\n",
        "                                             )\n",
        "            self.input_ids.append(torch.squeeze(encodings['input_ids'], 0))\n",
        "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'], 0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "# Create the dataset\n",
        "dataset = RecipeDataset(data, tokenizer)\n",
        "\n",
        "# Ensure dataset is large enough for a split\n",
        "min_dataset_size = 3\n",
        "if len(dataset) < min_dataset_size:\n",
        "    raise ValueError(f\"Dataset must contain at least {min_dataset_size} items, but contains {len(dataset)}.\")\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Ensure at least one item in both sets\n",
        "if train_size == 0:\n",
        "    train_size = 1\n",
        "    val_size = len(dataset) - train_size\n",
        "elif val_size == 0:\n",
        "    val_size = 1\n",
        "    train_size = len(dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create the DataLoaders for our training and validation datasets\n",
        "# Get training samples in random order\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Get validation samples sequentially\n",
        "validation_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    sampler=SequentialSampler(val_dataset),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Print the first batch from the train_dataloader to verify\n",
        "for batch in train_dataloader:\n",
        "    input_ids, attn_masks = batch\n",
        "    print(f\"input_ids: {input_ids}\")\n",
        "    print(f\"attn_masks: {attn_masks}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzP_1-er9MyB",
        "outputId": "c518c256-333c-41bf-ad28-02caf8796805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: tensor([[30119,    64,    11, 24240,    11, 37792, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
            "        [20970,    11, 16567,    11, 40377, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
            "attn_masks: tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model and move it to the device\n",
        "configuration = GPT2Config.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
        "model = model.to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Training parameters\n",
        "epochs = 1\n",
        "learning_rate = 2e-5\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "optim = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs  # [no batches] x [no epochs]\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attn_masks = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attn_masks = attn_masks.to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attn_masks, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Average training loss: {avg_train_loss}\")\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids, attn_masks = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attn_masks = attn_masks.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attn_masks, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(validation_dataloader)\n",
        "    print(f\"Average validation loss: {avg_eval_loss}\")"
      ],
      "metadata": {
        "id": "gyIEpF8a4VKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c5b0eb-aae4-42df-c7ba-bb3f19783480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 9.164665222167969\n",
            "Average validation loss: 10.879677772521973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_i in range(0, epochs):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels    = batch[0].to(device)\n",
        "        b_masks     = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model( input_ids = b_input_ids, labels = b_labels,\n",
        "                         attention_mask = b_masks, token_type_ids = None )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            model.eval()\n",
        "            print(infer(\"eggs, flour, butter, sugar\"))\n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "IytBJuVO4rQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels    = batch[0].to(device)\n",
        "        b_masks     = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs  = model(input_ids = b_input_ids, labels = b_labels\n",
        "                             attention_mask = b_masks)\n",
        "            loss = outputs[0]"
      ],
      "metadata": {
        "id": "wdLxbVX44ek3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "40BpgARX4pdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and move it to the device\n",
        "configuration = GPT2Config.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
        "model = model.to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Initialize optimizer and scheduler\n",
        "optim = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Define inference function\n",
        "def infer(prompt):\n",
        "    input = f\"Ingredients: {prompt.strip()}\"\n",
        "    input = tokenizer(input, return_tensors=\"pt\")\n",
        "    input_ids = input[\"input_ids\"]\n",
        "    attention_mask = input[\"attention_mask\"]\n",
        "\n",
        "    output = model.generate(input_ids.to(device),\n",
        "                            attention_mask=attention_mask.to(device),\n",
        "                            max_new_tokens=max_length,\n",
        "                            do_sample=True, top_k=50, top_p=0.85)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return output\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, epochs):\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids=b_input_ids, labels=b_labels,\n",
        "                        attention_mask=b_masks, token_type_ids=None)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Get sample every 100 batches.\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            model.eval()\n",
        "            print(infer(\"eggs, flour, butter, sugar\"))\n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Average training loss: {avg_train_loss}\")\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=b_input_ids, labels=b_labels,\n",
        "                            attention_mask=b_masks)\n",
        "            loss = outputs.loss\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(validation_dataloader)\n",
        "    print(f\"Average validation loss: {avg_eval_loss}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Reload the model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "LLhvdni0438O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade53804-e1a3-4668-8e11-578f6f6b990a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 8.958427906036377\n",
            "Average validation loss: 10.880024909973145\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer(\"eggs, mushroom, butter, sugar\")"
      ],
      "metadata": {
        "id": "xQB9P2UA489V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1c16acc1-0fdc-4687-b078-ea4283aa44a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ingredients: eggs, mushroom, butter, sugar, cayenne, garlic, salt, cinnamon, oregano, parsley, ginger, paprika, cayenne pepper, cumin, oregano, ginger, garlic, onion, garlic powder, and herbs.\\n\\nDirections: Combine all ingredients into a blender and blend until smooth. You will notice that the eggs start to come out a little thick and creamy, but not that thick at all. Add more water, salt, and spices in an effort to get it to taste exactly like you expect it to, and add in as much water as you need.\\n\\nIf you're using only the eggs, use more water than you need, as the water is the same amount of water you used when you began baking. Use about 2/3 cup for each cup of butter you use.\\n\\nIf you are using the whole ingredients for this recipe, use\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}