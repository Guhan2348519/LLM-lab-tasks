{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb61vcA1r3XyKRdMlIYaEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guhan2348519/LLM-lab-tasks/blob/main/2348519_LLM_Lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w4FSKmjDnME"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "def load_model_and_tokenizer(model_name_or_path):\n",
        "    \"\"\"\n",
        "    Load a fine-tuned model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model_name_or_path (str): Path to the model directory or model name from Hugging Face Model Hub.\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded model object.\n",
        "        tokenizer: Loaded tokenizer object.\n",
        "    \"\"\"\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "    # Load the model\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_name_or_path = \"your-fine-tuned-model-directory-or-name\"\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name_or_path)\n",
        "\n",
        "    # Test the model and tokenizer\n",
        "    input_text = \"Translate this text.\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"Generated text:\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "script that loads a fine-tuned model and its corresponding tokenizer using the Hugging Face Transformers library. This script assumes that the model and tokenizer are stored locally or can be downloaded from the Hugging Face Model Hub."
      ],
      "metadata": {
        "id": "_DRionwWEgHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForVision2Seq, AutoTokenizer, CLIPProcessor\n",
        "from PIL import Image\n",
        "\n",
        "def process_text_input(text, tokenizer):\n",
        "    \"\"\"\n",
        "    Process the text input.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        tokenizer: The tokenizer for the model.\n",
        "\n",
        "    Returns:\n",
        "        dict: Tokenized text input.\n",
        "    \"\"\"\n",
        "    return tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "def process_image_input(image_path, processor):\n",
        "    \"\"\"\n",
        "    Process the image input.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        processor: The processor for the model (e.g., CLIPProcessor).\n",
        "\n",
        "    Returns:\n",
        "        dict: Processed image input.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    return processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "def handle_multimodal_input(text, image_path, model, tokenizer, processor):\n",
        "    \"\"\"\n",
        "    Handle multimodal input by processing text and image, then generating a response.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text input.\n",
        "        image_path (str): Path to the image file.\n",
        "        model: Loaded multimodal model.\n",
        "        tokenizer: Tokenizer for the model.\n",
        "        processor: Processor for image input.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text from the model.\n",
        "    \"\"\"\n",
        "    text_input = process_text_input(text, tokenizer)\n",
        "    image_input = process_image_input(image_path, processor)\n",
        "\n",
        "    # Combine text and image inputs\n",
        "    inputs = {**text_input, **image_input}\n",
        "\n",
        "    # Generate output\n",
        "    outputs = model.generate(**inputs)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_name_or_path = \"your-multimodal-model-directory-or-name\"\n",
        "    model = AutoModelForVision2Seq.from_pretrained(model_name_or_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "    processor = CLIPProcessor.from_pretrained(model_name_or_path)\n",
        "\n",
        "    text_input = \"Describe this image.\"\n",
        "    image_path = \"path/to/your/image.jpg\"\n",
        "\n",
        "    result = handle_multimodal_input(text_input, image_path, model, tokenizer, processor)\n",
        "    print(\"Generated response:\", result)"
      ],
      "metadata": {
        "id": "2zIi3wWeEhia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If out model supports multimodal inputs (e.g., text, images, or audio), you'll need a script to process and integrate these inputs before passing them to the model. Below is an example script that handles text and image inputs. The script uses the transformers library for text and a pre-trained vision model for images."
      ],
      "metadata": {
        "id": "CC6hgs-8Erdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loading the Fine-Tuned Model and Tokenizer:\n",
        "   - The script loads a model and tokenizer using Hugging Face's `AutoModelForSeq2SeqLM` and `AutoTokenizer`.\n",
        "   - The `load_model_and_tokenizer` function abstracts the loading logic, which you can use for any model.\n",
        "\n",
        "2. Handling Multimodal Inputs:\n",
        "   - The script demonstrates how to handle text and image inputs.\n",
        "   - It uses a hypothetical multimodal model capable of taking both types of inputs.\n",
        "   - The script processes the text and image inputs separately and then combines them for model inference."
      ],
      "metadata": {
        "id": "PfKCf4XbFFko"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ITdCAABNFMSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}